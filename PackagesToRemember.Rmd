---
title: "Packages to Remember"
author: "will"
date: "6/19/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Packages to Remember **EARLY DRAFT**
What packages to I want to remember? Examples from each.

In general, I'm seeking to stay in the tidyverse for efficiency; venturing out when there are compelling reasons to do so.

## Ingest
Getting the data in from other sources.

### Files

```{r}
library(readr)
library(readxl)
library(jsonlite) # when I have to work with JSON files
library(haven) # not really used; but like knowing it exists if needed
```
#### readr
This is a workhorse package; reading (parsing) from flat files (csv, tab-delim, etc) to tibbles.

Can read directly from URLs. Let's read some sample sales data. 
```{r message=FALSE}
sales <- read_csv("http://samplecsvs.s3.amazonaws.com/SalesJan2009.csv")
```

#### jsonlite
From Getting Started:

> The jsonlite package is a JSON parser/generator optimized for the web. Its main strength is that it implements a bidirectional mapping between JSON data and the most important R data types. Thereby we can convert between R objects and JSON without loss of type or information, and without the need for any manual data munging. This is ideal for interacting with web APIs, or to build pipelines where data structures seamlessly flow in and out of R using JSON.

### Databases
Sometimes I'm in the happy (for me) position of being able to read direct from database. Then some more libraries are needed.
```{r}
library(DBI) # foundational for both odbc and RSQLite
library(odbc)
library(RSQLite)

sales_db <- dbConnect(SQLite(),"salesDB.sqlite")
dbWriteTable(sales_db,"Sales", as.data.frame(sales))
dbListTables(sales_db)
dbGetQuery(sales_db,'SELECT * FROM Sales LIMIT 10')
#clean-up
dbDisconnect(sales_db)
unlink("salesDB.sqlite")
```

The Azure libraries are pretty great to have handy when connecting to Azure-land
```{r}
library(AzureAuth)
library(AzureRMR)
library(AzureStor)
```

## Reference Data/Libraries
```{r}
library(countrycode)
```


## Process/Transform
This the core data work. Sometimes transformed data is the goal of the exercise; sometimes the subsequent anaysis is. 

#### dplyr
```{r}
library(dplyr) 
library(lubridate)
library(tidyr)
glimpse(sales) # Use dplyr to take a look at the sales data
sales %>% 
        filter(Payment_Type == "Mastercard") %>%
        mutate(Transaction_date = floor_date(mdy_hm(Transaction_date),unit="day")) %>%
        unite("addr",City,State,Country,sep=", ") %>%
        group_by(Transaction_date, Product) %>% 
        summarize(tot_rev = sum(Price)) %>%
        arrange(Transaction_date)
```

```{r}
library(broom)
library(purrr)
library(stringr)
```

## Analysis
 * Core tools
 * Forecasting
        * forecast
        * modelr
        * CausalImpact
        * mclust
```{r}
library(rsample) # makes sampling easy, pretty new, part of tidymodeling set
                 # modelr has many of the same fuctions, slowly being replaced
```

## Visualization
```{r}
library(ggplot2)
library(plotly)
library(DT)
```

### Dimensional Data
The ridgeline library ggridges makes adding ordered data easy. 
```{r}
library(ggridges)
```

## Presentions
*Markdown
*Word
*PowerPoint
*Shiny
```{r}
library(shiny)
library(shinythemes)
library(knitr)
library(kableExtra)
```

## Storing data

### Locally

### Remote/Cloud

# Saving work with Git

## This workbook

# Saving environment with Docker



